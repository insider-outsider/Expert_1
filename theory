1ï¸âƒ£ What do we mean by the receptive field of a filter?

Itâ€™s the region of the input image that affects one neuronâ€™s activation in a feature map.
In early layers, receptive fields are small (3Ã—3), but deeper layers combine them â†’ larger receptive fields covering more of the image.

ğŸ§  2ï¸âƒ£ What do we mean by the spatial hierarchies of a CNN?

CNNs build feature hierarchies:

Early layers learn simple patterns (edges, corners).

Deeper layers combine them into complex shapes (faces, objects).
This hierarchical learning helps CNNs understand both local and global image structures.

ğŸ§  3ï¸âƒ£ What do we mean by the translation invariance of CNNs?

Translation invariance means the model can recognize an object even if it moves in the image.
Pooling and shared filters make CNNs insensitive to small shifts in object position.

ğŸ§  4ï¸âƒ£ Effect of kernel size on output size and parameters

Larger kernel (5Ã—5) â†’ more parameters, smaller output (covers bigger area).

Smaller kernel (3Ã—3) â†’ fewer parameters, preserves spatial detail.
âœ… Usually, CNNs use 3Ã—3 kernels for efficiency and depth.

ğŸ§  5ï¸âƒ£ Typical structure for CNNs & typical values

Common order:

Conv â†’ ReLU â†’ Pool â†’ Conv â†’ ReLU â†’ Pool â†’ Flatten â†’ Linear â†’ Output


Typical values:

Kernel = 3Ã—3

Stride = 1 (Conv), 2 (Pool)

Padding = 1

Filters = 16, 32, 64, 128 (increasing depth)

ğŸ§  6ï¸âƒ£ Two properties that make CNNs better for images than fully connected networks

Local connectivity: Each neuron looks only at a small region (efficient).

Parameter sharing: One filter is used across the entire image â†’ fewer weights, better generalization.

ğŸ§  7ï¸âƒ£ Explain/use padding.

Padding adds extra border pixels (often zeros) around the image so that convolution doesnâ€™t shrink the output size and the filter can see border features.
E.g., padding=1 with kernel=3 keeps size same.

ğŸ§  8ï¸âƒ£ Explain/use stride.

Stride = how many pixels the filter moves at each step.

Stride=1 â†’ detailed, larger output.

Stride=2 â†’ faster, smaller output.
It controls how much the feature map is downsampled.

ğŸ§  9ï¸âƒ£ What is the purpose of a pooling layer?

Pooling reduces feature map size (H, W) while keeping key information.

MaxPooling picks strongest feature.
It improves translation invariance and reduces computation.

ğŸ§  ğŸ”Ÿ Difference between parameter and hyperparameter
Type	Description	Examples
Parameter	Learned by model during training	Weights, biases
Hyperparameter	Set manually before training	Learning rate, kernel size, epochs
ğŸ§  11ï¸âƒ£ What are training, validation, and test datasets used for?

Training set: For learning model parameters.

Validation set: For tuning hyperparameters & avoiding overfitting.

Test set: For final performance evaluation on unseen data.

ğŸ§  12ï¸âƒ£ What is a baseline and how is it used?

A baseline is a simple reference model (like random guess or logistic regression) used to compare your CNNâ€™s performance.
If CNN doesnâ€™t outperform baseline â†’ itâ€™s not effective.

ğŸ§  13ï¸âƒ£ What are some ways to deal with overfitting?

Add more data or apply data augmentation.

Use Dropout and Regularization (L2, weight decay).

Use Early stopping during training.

Reduce model complexity or apply BatchNorm.
